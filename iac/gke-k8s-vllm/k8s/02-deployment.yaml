apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama3
  template:
    metadata:
      labels:
        app: vllm-llama3
    spec:
      # This toleration allows the pod to be scheduled on our tainted GPU nodes
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: vllm-container
        image: vllm/vllm-openai:latest
        # Command to start the server with Llama 3
        args:
          - "--model"
          - "meta-llama/Meta-Llama-3-8B-Instruct"
          - "--tensor-parallel-size"
          - "1"
        env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-secret
                key: token
        ports:
        - containerPort: 8000
        # This is CRITICAL: it requests 1 GPU for the pod
        resources:
          limits:
            nvidia.com/gpu: 1
